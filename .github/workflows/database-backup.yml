name: Database Backup

on:
  # Run daily at 3 AM UTC
  schedule:
    - cron: '0 3 * * *'
  
  # Allow manual trigger
  workflow_dispatch:

jobs:
  backup:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Install PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client
      
      - name: Create backup
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          mkdir -p backups
          TIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)
          BACKUP_FILE="backups/backup-${TIMESTAMP}.sql"
          
          echo "Creating backup: ${BACKUP_FILE}"
          pg_dump "${DATABASE_URL}" > "${BACKUP_FILE}"
          
          # Check backup size
          SIZE=$(du -h "${BACKUP_FILE}" | cut -f1)
          echo "Backup size: ${SIZE}"
          
          # Compress backup
          gzip "${BACKUP_FILE}"
          echo "Compressed backup: ${BACKUP_FILE}.gz"
      
      - name: Upload backup artifact
        uses: actions/upload-artifact@v4
        with:
          name: database-backup-${{ github.run_number }}
          path: backups/*.sql.gz
          retention-days: 90
      
      # Optional: Upload to S3 (uncomment and configure if needed)
      # - name: Upload to S3
      #   env:
      #     AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      #     AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      #     AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
      #   run: |
      #     aws s3 cp backups/*.sql.gz s3://${AWS_S3_BUCKET}/fatos-pro-backups/
      
      - name: Notify on failure
        if: failure()
        run: |
          echo "‚ùå Backup failed!"
          echo "Check the workflow logs for details"
